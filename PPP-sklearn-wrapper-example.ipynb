{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data from https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Alternatively X and y can be obtained directly from the frame attribute:\n",
    "# X = titanic.frame.drop('survived', axis=1)\n",
    "# y = titanic.frame['survived']\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical_features', numeric_transformer, numeric_features),\n",
    "        ('categorical_features', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class SwappedValues:\n",
    "\n",
    "    def __init__(self, fraction, column_pair):\n",
    "        self.fraction = fraction\n",
    "        self.column_pair = column_pair\n",
    "\n",
    "    def __call__(self, clean_df):\n",
    "        # we operate on a copy of the data\n",
    "        df = clean_df.copy(deep=True)\n",
    "\n",
    "        (column_a, column_b) = self.column_pair\n",
    "\n",
    "        values_of_column_a = list(df[column_a])\n",
    "        values_of_column_b = list(df[column_b])\n",
    "\n",
    "        for index in range(0, len(values_of_column_a)):\n",
    "            if random.random() < self.fraction:\n",
    "                temp_value = values_of_column_a[index]\n",
    "                values_of_column_a[index] = values_of_column_b[index]\n",
    "                values_of_column_b[index] = temp_value\n",
    "\n",
    "        df[column_a] = values_of_column_a\n",
    "        df[column_b] = values_of_column_b\n",
    "\n",
    "        return df\n",
    "\n",
    "class Outliers:\n",
    "\n",
    "    def __init__(self, fraction, columns):\n",
    "        self.fraction = fraction\n",
    "        self.columns = columns\n",
    "\n",
    "    def __call__(self, clean_df):\n",
    "        # we operate on a copy of the data\n",
    "        df = clean_df.copy(deep=True)\n",
    "        # means = {column: np.mean(df[column]) for column in self.columns}\n",
    "        stddevs = {column: np.std(df[column]) for column in self.columns}\n",
    "        scales = {column: random.uniform(1, 5) for column in self.columns}\n",
    "\n",
    "        if self.fraction > 0:\n",
    "            for column in self.columns:\n",
    "                rows = np.random.uniform(size=len(df))<self.fraction\n",
    "                noise = np.random.normal(0, scales[column] * stddevs[column], size=rows.sum())\n",
    "                df.loc[rows, column] += noise\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class Scaling:\n",
    "\n",
    "    def __init__(self, fraction, columns):\n",
    "        self.fraction = fraction\n",
    "        self.columns = columns\n",
    "\n",
    "    def __call__(self, clean_df):\n",
    "        # we operate on a copy of the data\n",
    "        df = clean_df.copy(deep=True)\n",
    "\n",
    "        scale_factor = np.random.choice([10, 100, 1000])\n",
    "        \n",
    "        if self.fraction > 0:\n",
    "            for column in self.columns:\n",
    "                rows = np.random.uniform(size=len(df))<self.fraction\n",
    "                df.loc[rows, column] *= scale_factor\n",
    "\n",
    "        return df\n",
    "\n",
    "class MissingValuesHighEntropy:\n",
    "\n",
    "    def __init__(self, \n",
    "                    fraction, \n",
    "                    model, \n",
    "                    categorical_columns, \n",
    "                    numerical_columns,\n",
    "                    categorical_value_to_put_in='NULL',\n",
    "                    numerical_value_to_put_in=0):\n",
    "        self.fraction = fraction\n",
    "        self.model = model\n",
    "        self.categorical_value_to_put_in = categorical_value_to_put_in\n",
    "        self.numerical_value_to_put_in = numerical_value_to_put_in\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.numerical_columns = numerical_columns\n",
    "\n",
    "    def __call__(self, clean_df):\n",
    "        # we operate on a copy of the data\n",
    "        df = clean_df.copy(deep=True)\n",
    "        df[self.categorical_columns] = df[self.categorical_columns].astype(str) \n",
    "        probas = self.model.predict_proba(df)\n",
    "        # for samples with the smallest maximum probability \n",
    "        # the model is most uncertain\n",
    "        cutoff = int(len(df) * (1-self.fraction))\n",
    "        least_confident = probas.max(axis=1).argsort()[-cutoff:]\n",
    "        for c in self.categorical_columns:\n",
    "#             df.loc[df.index[least_confident], c].add_categories(self.categorical_value_to_put_in)\n",
    "            df.loc[df.index[least_confident], c] = self.categorical_value_to_put_in\n",
    "        for c in self.numerical_columns:\n",
    "            df.loc[df.index[least_confident], c] = self.numerical_value_to_put_in\n",
    "\n",
    "        return df\n",
    "\n",
    "class MissingValuesLowEntropy:\n",
    "\n",
    "    def __init__(self, \n",
    "                    fraction, \n",
    "                    model, \n",
    "                    categorical_columns, \n",
    "                    numerical_columns,\n",
    "                    categorical_value_to_put_in='NULL',\n",
    "                    numerical_value_to_put_in=0):\n",
    "        self.fraction = fraction\n",
    "        self.model = model\n",
    "        self.categorical_value_to_put_in = categorical_value_to_put_in\n",
    "        self.numerical_value_to_put_in = numerical_value_to_put_in\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.numerical_columns = numerical_columns\n",
    "\n",
    "    def __call__(self, clean_df):\n",
    "        # we operate on a copy of the data\n",
    "        df = clean_df.copy(deep=True)\n",
    "        df[self.categorical_columns] = df[self.categorical_columns].astype(str) \n",
    "        probas = self.model.predict_proba(df)\n",
    "        # for samples with the smallest maximum probability \n",
    "        # the model is most uncertain\n",
    "        cutoff = int(len(df) * (1-self.fraction))\n",
    "        most_confident = probas.max(axis=1).argsort()[:cutoff]\n",
    "        for c in self.categorical_columns:\n",
    "#             df.loc[df.index[least_confident], c].add_categories(self.categorical_value_to_put_in)\n",
    "            df.loc[df.index[most_confident], c] = self.categorical_value_to_put_in\n",
    "        for c in self.numerical_columns:\n",
    "            df.loc[df.index[most_confident], c] = self.numerical_value_to_put_in\n",
    "\n",
    "        return df\n",
    "\n",
    "class PipelineWithPPP:\n",
    "\n",
    "    def __init__(self, \n",
    "                pipeline, \n",
    "                num_repetitions=10, \n",
    "                perturbation_fractions=np.linspace(0,1,11)):\n",
    "        self.pipeline = pipeline\n",
    "        self.num_repetitions = num_repetitions\n",
    "        self.perturbation_fractions = perturbation_fractions\n",
    "        # assuming the first step is a ColumnTransformer with transformers named \n",
    "        # 'categorical_features' or 'numerical_features'\n",
    "        self.categorical_features = []\n",
    "        self.numerical_features = []\n",
    "        for t in pipeline.steps[0][1].transformers:\n",
    "            if t[0]=='categorical_features':\n",
    "                self.categorical_features = t[2]\n",
    "            if t[0]=='numerical_features':\n",
    "                self.numerical_features = t[2]\n",
    "        print(f'Registered categorical columns: {self.categorical_features}')\n",
    "        print(f'Registered numerical columns: {self.numerical_features}')\n",
    "\n",
    "        \n",
    "        self.perturbations = []\n",
    "        for _ in range(self.num_repetitions):\n",
    "            for fraction in self.perturbation_fractions:\n",
    "                \n",
    "                numerical_column_pairs = list(itertools.combinations(self.numerical_features, 2))\n",
    "                swap_affected_column_pair = random.choice(numerical_column_pairs)\n",
    "                affected_numeric_column = random.choice(self.numerical_features)\n",
    "                affected_categorical_column = np.random.choice(self.categorical_features)\n",
    "\n",
    "                self.perturbations += [\n",
    "                    ('swapped', SwappedValues(fraction, swap_affected_column_pair)),\n",
    "                    ('scaling', Scaling(fraction, [affected_numeric_column])),\n",
    "                    ('outlier', Outliers(fraction, [affected_numeric_column])),\n",
    "                    ('missing_high_entropy', MissingValuesHighEntropy(fraction, pipeline, [affected_categorical_column], [affected_numeric_column])),\n",
    "                    ('missing_low_entropy', MissingValuesLowEntropy(fraction, pipeline, [affected_categorical_column], [affected_numeric_column])),\n",
    "                ]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_ppp_features(predictions):\n",
    "        probs_class_a = np.transpose(predictions)[0]\n",
    "        features_a = np.percentile(probs_class_a, np.arange(0, 101, 5))\n",
    "        if predictions.shape[-1] > 1:\n",
    "            probs_class_b = np.transpose(predictions)[1]\n",
    "            features_b = np.percentile(probs_class_b, np.arange(0, 101, 5))\n",
    "            return np.concatenate((features_a, features_b), axis=0)\n",
    "        else:\n",
    "            return features_a\n",
    "\n",
    "    def fit_ppp(self, X_df, y):\n",
    "\n",
    "        print(\"Generating perturbed training data...\")\n",
    "        meta_features = []\n",
    "        meta_scores = []\n",
    "        for perturbation in self.perturbations:\n",
    "            df_perturbed = perturbation[1](X_df)\n",
    "      \n",
    "            predictions = self.pipeline.predict_proba(df_perturbed)\n",
    "            \n",
    "            meta_features.append(self.compute_ppp_features(predictions))\n",
    "            meta_scores.append(self.pipeline.score(df_perturbed, y))\n",
    " \n",
    "        param_grid = {\n",
    "            'learner__n_estimators': np.arange(5, 20, 5),\n",
    "            'learner__criterion': ['mae']\n",
    "        }\n",
    "\n",
    "        meta_regressor_pipeline = Pipeline([\n",
    "           ('scaling', StandardScaler()),\n",
    "           ('learner', RandomForestRegressor(criterion='mae'))\n",
    "        ])\n",
    "\n",
    "        print(\"Training performance predictor...\")\n",
    "        self.meta_regressor = GridSearchCV(\n",
    "                                meta_regressor_pipeline, \n",
    "                                param_grid, \n",
    "                                scoring='neg_mean_absolute_error')\\\n",
    "                                    .fit(meta_features, meta_scores)\n",
    "\n",
    "    def predict_ppp(self, X_df):\n",
    "        meta_features = self.compute_ppp_features(self.pipeline.predict_proba(X_df))\n",
    "        return self.meta_regressor.predict(meta_features.reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered categorical columns: ['embarked', 'sex', 'pclass']\n",
      "Registered numerical columns: ['age', 'fare']\n",
      "Generating perturbed training data...\n",
      "Training performance predictor...\n",
      "Predicted score: 0.7844, true score 0.7901\n"
     ]
    }
   ],
   "source": [
    "# from ppp import PipelineWithPPP\n",
    "ppp = PipelineWithPPP(clf, num_repetitions=100)\n",
    "ppp.fit_ppp(X_train, y_train)\n",
    "print(f'Predicted score: {ppp.predict_ppp(X_test):.4f}, true score {clf.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
